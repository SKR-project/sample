{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGtv/jmJcY0W33EVCaI4rb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SKR-project/NLP-model-hackathon-/blob/main/NLP_model_%5BHackathon%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUBrtE83qPz8",
        "outputId": "1b0dcf82-8275-417d-a611-9498f92b3751"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1874/1874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 175ms/step - accuracy: 0.7110 - loss: 0.9636 - val_accuracy: 0.7484 - val_loss: 0.7361\n",
            "Epoch 2/5\n",
            "\u001b[1m1874/1874\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 168ms/step - accuracy: 0.7625 - loss: 0.7020 - val_accuracy: 0.7555 - val_loss: 0.7084\n",
            "Epoch 3/5\n",
            "\u001b[1m 498/1874\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:43\u001b[0m 162ms/step - accuracy: 0.7848 - loss: 0.6233"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import joblib\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv('/content/sample_data/train.csv')\n",
        "\n",
        "# Text cleaning and preprocessing function (using Lemmatization)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\W', ' ', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        words = nltk.word_tokenize(text)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "        words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        return ' '.join(words)\n",
        "    else:\n",
        "        return ''  # Or any other suitable default value\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "train_data['cleaned_text'] = train_data['crimeaditionalinfo'].apply(preprocess_text)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X = train_data['cleaned_text']\n",
        "y = train_data['category']\n",
        "\n",
        "# Convert labels to numerical format using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization and Padding (for deep learning models)\n",
        "max_words = 5000  # Maximum number of words to consider in the vocabulary\n",
        "max_len = 100     # Maximum length of the sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Define the GRU Model with Bidirectional Layer and Increased Epochs\n",
        "embedding_dim = 100  # Increased embedding dimension\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
        "model.add(Bidirectional(GRU(64, return_sequences=False)))  # Bidirectional GRU layer\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))  # Increased hidden layer size\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Output layer\n",
        "\n",
        "# Compile the model with a lower learning rate\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model with more epochs\n",
        "history = model.fit(X_train_padded, y_train, epochs=5, validation_split=0.2, batch_size=32)  # Increased epochs\n",
        "\n",
        "# Make predictions\n",
        "y_pred_proba = model.predict(X_test_padded)\n",
        "y_pred = y_pred_proba.argmax(axis=1)\n",
        "\n",
        "# Accuracy Measurement\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Print evaluation results\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'Precision: {precision:.4f}')\n",
        "print(f'Recall: {recall:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Confusion matrix visualization\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix as a DataFrame\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "cm_df = pd.DataFrame(cm, index=label_encoder.classes_, columns=label_encoder.classes_)\n",
        "print(\"\\nConfusion Matrix DataFrame:\\n\", cm_df)\n",
        "\n",
        "# Confusion matrix as a bar chart\n",
        "cm_sum = cm.sum(axis=1)  # Sum the true instances for each class\n",
        "cm_correct = np.diag(cm)  # Correctly predicted instances (diagonal elements)\n",
        "cm_incorrect = cm_sum - cm_correct  # Incorrectly predicted instances\n",
        "\n",
        "# Create a DataFrame for plotting\n",
        "cm_plot_df = pd.DataFrame({\n",
        "    'Class': label_encoder.classes_,\n",
        "    'Correct': cm_correct,\n",
        "    'Incorrect': cm_incorrect\n",
        "})\n",
        "\n",
        "# Plot the bar chart\n",
        "cm_plot_df.set_index('Class').plot(kind='bar', stacked=True, color=['green', 'red'], figsize=(10, 6))\n",
        "plt.title('Correct vs Incorrect Predictions per Class')\n",
        "plt.ylabel('Number of Instances')\n",
        "plt.xlabel('Class')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Save the model and tokenizer for future use\n",
        "model.save('gru_model.h5')\n",
        "joblib.dump(tokenizer, 'tokenizer.pkl')\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tkinter as tk\n",
        "from tkinter import ttk, filedialog, messagebox\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from tensorflow.keras.models import load_model\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the saved model, tokenizer, and label encoder\n",
        "model = load_model('gru_model.h5')\n",
        "tokenizer = joblib.load('tokenizer.pkl')\n",
        "label_encoder = joblib.load('label_encoder.pkl')\n",
        "\n",
        "# NLTK setup\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Preprocess function\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\W', ' ', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        words = nltk.word_tokenize(text)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        words = [word for word in words if word not in stop_words]\n",
        "        words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        return ' '.join(words)\n",
        "    else:\n",
        "        return ''  # Or any other suitable default value\n",
        "\n",
        "# Prediction function\n",
        "def predict_category(text):\n",
        "    processed_text = preprocess_text(text)\n",
        "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=100, padding='post')  # Adjust maxlen if needed\n",
        "    prediction_proba = model.predict(padded_sequence)\n",
        "    prediction = prediction_proba.argmax(axis=1)\n",
        "    predicted_category = label_encoder.inverse_transform(prediction)\n",
        "    return predicted_category[0]\n",
        "\n",
        "# Function to handle predictions for multiple entries in a CSV file\n",
        "def predict_from_file(filepath):\n",
        "    try:\n",
        "        data = pd.read_csv(filepath)\n",
        "        if 'crimeaditionalinfo' not in data.columns:\n",
        "            messagebox.showerror(\"Error\", \"The file must contain 'crimeaditionalinfo' column.\")\n",
        "            return None\n",
        "        data['cleaned_text'] = data['crimeaditionalinfo'].apply(preprocess_text)\n",
        "        sequences = tokenizer.texts_to_sequences(data['cleaned_text'])\n",
        "        padded_sequences = pad_sequences(sequences, maxlen=100, padding='post')\n",
        "        predictions_proba = model.predict(padded_sequences)\n",
        "        predictions = predictions_proba.argmax(axis=1)\n",
        "        data['predicted_category'] = label_encoder.inverse_transform(predictions)\n",
        "        save_path = filedialog.asksaveasfilename(defaultextension=\".csv\",\n",
        "                                                 filetypes=[(\"CSV files\", \"*.csv\"), (\"All files\", \"*.*\")])\n",
        "        if save_path:\n",
        "            data.to_csv(save_path, index=False)\n",
        "            messagebox.showinfo(\"Success\", f\"Predictions saved to {save_path}\")\n",
        "    except Exception as e:\n",
        "        messagebox.showerror(\"Error\", str(e))\n",
        "\n",
        "# GUI setup\n",
        "root = tk.Tk()\n",
        "root.title(\"NLP Model - Category Prediction\")\n",
        "root.geometry(\"500x400\")\n",
        "\n",
        "# Input label and text box\n",
        "input_label = tk.Label(root, text=\"Enter text for prediction:\")\n",
        "input_label.pack(pady=5)\n",
        "\n",
        "text_entry = tk.Text(root, height=5, width=50)\n",
        "text_entry.pack(pady=5)\n",
        "\n",
        "# Prediction display\n",
        "result_label = tk.Label(root, text=\"Predicted Category: \", font=(\"Arial\", 14))\n",
        "result_label.pack(pady=10)\n",
        "\n",
        "# Predict button function\n",
        "def predict_button_click():\n",
        "    text = text_entry.get(\"1.0\", tk.END).strip()\n",
        "    if text:\n",
        "        prediction = predict_category(text)\n",
        "        result_label.config(text=f\"Predicted Category: {prediction}\")\n",
        "    else:\n",
        "        messagebox.showwarning(\"Input Error\", \"Please enter some text for prediction.\")\n",
        "\n",
        "# Predict button\n",
        "predict_button = tk.Button(root, text=\"Predict\", command=predict_button_click)\n",
        "predict_button.pack(pady=5)\n",
        "\n",
        "# File prediction button function\n",
        "def file_button_click():\n",
        "    file_path = filedialog.askopenfilename(filetypes=[(\"CSV files\", \"*.csv\")])\n",
        "    if file_path:\n",
        "        predict_from_file(file_path)\n",
        "\n",
        "# File prediction button\n",
        "file_button = tk.Button(root, text=\"Predict from CSV File\", command=file_button_click)\n",
        "file_button.pack(pady=5)\n",
        "\n",
        "# Run the application\n",
        "root.mainloop()\n"
      ],
      "metadata": {
        "id": "SpsLe2L4rDjX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}